{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.initializers import *\n",
    "from keras.optimizers import *\n",
    "import keras.backend as K\n",
    "from keras.callbacks import *\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import re\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Window</th>\n",
       "      <th>Start</th>\n",
       "      <th>End</th>\n",
       "      <th>Word</th>\n",
       "      <th>File</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pandemic Spread other David Metzgar Darcie Baynes</td>\n",
       "      <td>303</td>\n",
       "      <td>308</td>\n",
       "      <td>David</td>\n",
       "      <td>PMC3020883</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Health Research Center San Diego California Re...</td>\n",
       "      <td>503</td>\n",
       "      <td>512</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>PMC3020883</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Center San Diego California Received 30 June</td>\n",
       "      <td>514</td>\n",
       "      <td>524</td>\n",
       "      <td>California</td>\n",
       "      <td>PMC3020883</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>identified in Southern California in March 2009</td>\n",
       "      <td>729</td>\n",
       "      <td>739</td>\n",
       "      <td>California</td>\n",
       "      <td>PMC3020883</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Southern California in March 2009 One was</td>\n",
       "      <td>743</td>\n",
       "      <td>748</td>\n",
       "      <td>March</td>\n",
       "      <td>PMC3020883</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Research Center in San Diego CA on 13</td>\n",
       "      <td>1568</td>\n",
       "      <td>1577</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>PMC3020883</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>in San Diego CA on 13 April</td>\n",
       "      <td>1579</td>\n",
       "      <td>1581</td>\n",
       "      <td>CA</td>\n",
       "      <td>PMC3020883</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spectrometry PCR ESI- MS designed to amplify</td>\n",
       "      <td>1699</td>\n",
       "      <td>1701</td>\n",
       "      <td>MS</td>\n",
       "      <td>PMC3020883</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>potential On 28 March 2009 a throat</td>\n",
       "      <td>2444</td>\n",
       "      <td>2449</td>\n",
       "      <td>March</td>\n",
       "      <td>PMC3020883</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>illness FRI in Imperial County California This</td>\n",
       "      <td>2560</td>\n",
       "      <td>2568</td>\n",
       "      <td>Imperial</td>\n",
       "      <td>PMC3020883</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Window  Start   End  \\\n",
       "0  Pandemic Spread other David Metzgar Darcie Baynes    303   308   \n",
       "1  Health Research Center San Diego California Re...    503   512   \n",
       "2       Center San Diego California Received 30 June    514   524   \n",
       "3    identified in Southern California in March 2009    729   739   \n",
       "4          Southern California in March 2009 One was    743   748   \n",
       "5              Research Center in San Diego CA on 13   1568  1577   \n",
       "6                        in San Diego CA on 13 April   1579  1581   \n",
       "7       spectrometry PCR ESI- MS designed to amplify   1699  1701   \n",
       "8                potential On 28 March 2009 a throat   2444  2449   \n",
       "9     illness FRI in Imperial County California This   2560  2568   \n",
       "\n",
       "          Word        File  target  \n",
       "0        David  PMC3020883     NaN  \n",
       "1    San Diego  PMC3020883     NaN  \n",
       "2   California  PMC3020883     NaN  \n",
       "3   California  PMC3020883     NaN  \n",
       "4        March  PMC3020883     NaN  \n",
       "5    San Diego  PMC3020883     NaN  \n",
       "6           CA  PMC3020883     NaN  \n",
       "7           MS  PMC3020883     NaN  \n",
       "8        March  PMC3020883     NaN  \n",
       "9     Imperial  PMC3020883     NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = pd.read_csv(\"correct-train-2.tsv\", sep='\\t', header=0)\n",
    "test = pd.read_csv(\"test-new.tsv\", sep='\\t', header=0)\n",
    "test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Window\"] = train[\"Window\"].str.lower()\n",
    "test[\"Window\"] = test[\"Window\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "def clean_text(x):\n",
    "\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "\n",
    "train[\"Window\"] = train[\"Window\"].apply(lambda x: clean_text(x))\n",
    "test[\"Window\"] = test[\"Window\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some config values \n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = None # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 72 # max number of words in a question to use #99.99%\n",
    "\n",
    "## fill up the missing values\n",
    "X = train[\"Window\"].fillna(\"_na_\").values\n",
    "X_test = test[\"Window\"].fillna(\"_na_\").values\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features, filters='')\n",
    "tokenizer.fit_on_texts(list(X))\n",
    "\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "## Pad the sentences \n",
    "X = pad_sequences(X, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "## Get the target values\n",
    "Y = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "max_features = len(word_index)+1\n",
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '/data/glove/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if o.split(\" \")[0] in word_index)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix \n",
    "    \n",
    "def load_fasttext(word_index):    \n",
    "    EMBEDDING_FILE = '/data/fasttext/wiki-news-300d-1M.vec'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100 and o.split(\" \")[0] in word_index )\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '/data/paragram/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100 and o.split(\" \")[0] in word_index)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    \n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5826, 300)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_1 = load_glove(word_index)\n",
    "#embedding_matrix_2 = load_fasttext(word_index)\n",
    "#embedding_matrix_3 = load_para(word_index)\n",
    "embedding_matrix = embedding_matrix_1\n",
    "#embedding_matrix = np.mean((embedding_matrix_1, embedding_matrix_3), axis=0) \n",
    "#del embedding_matrix_1, embedding_matrix_3\n",
    "gc.collect()\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "            \n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capsule():\n",
    "    K.clear_session()       \n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(rate=0.1)(x)\n",
    "    x = Bidirectional(GRU(100, return_sequences=True, \n",
    "                                kernel_initializer=glorot_normal(seed=12300), recurrent_initializer=orthogonal(gain=1.0, seed=10000)))(x)\n",
    "\n",
    "    x = Capsule(num_capsule=10, dim_capsule=10, routings=4, share_weights=True)(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(100, activation=\"relu\", kernel_initializer=glorot_normal(seed=12300))(x)\n",
    "    x = Dropout(0.12)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=Adam(),)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_smart(y_true, y_pred):\n",
    "    args = np.argsort(y_pred)\n",
    "    tp = y_true.sum()\n",
    "    fs = (tp - np.cumsum(y_true[args[:-1]])) / np.arange(y_true.shape[0] + tp - 1, tp, -1)\n",
    "    res_idx = np.argmax(fs)\n",
    "    return 2 * fs[res_idx], (y_pred[args[res_idx]] + y_pred[args[res_idx + 1]]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 72, 300)           1747800   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 72, 300)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 72, 200)           240600    \n",
      "_________________________________________________________________\n",
      "capsule_1 (Capsule)          (None, 10, 10)            20000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 2,019,001\n",
      "Trainable params: 271,001\n",
      "Non-trainable params: 1,748,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4934 samples, validate on 1234 samples\n",
      "Epoch 1/6\n",
      " - 6s - loss: 0.7606 - val_loss: 0.5875\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.58754, saving model to weights_best.h5\n",
      "Epoch 2/6\n",
      " - 5s - loss: 0.5513 - val_loss: 0.4350\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.58754 to 0.43503, saving model to weights_best.h5\n",
      "Epoch 3/6\n",
      " - 5s - loss: 0.4124 - val_loss: 0.3432\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.43503 to 0.34324, saving model to weights_best.h5\n",
      "Epoch 4/6\n",
      " - 5s - loss: 0.3428 - val_loss: 0.3018\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.34324 to 0.30180, saving model to weights_best.h5\n",
      "Epoch 5/6\n",
      " - 5s - loss: 0.2939 - val_loss: 0.2724\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.30180 to 0.27241, saving model to weights_best.h5\n",
      "Epoch 6/6\n",
      " - 5s - loss: 0.2568 - val_loss: 0.2801\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.27241\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "Optimal F1: 0.8651 at threshold: 0.3250\n",
      "Train on 4934 samples, validate on 1234 samples\n",
      "Epoch 1/6\n",
      " - 6s - loss: 0.7510 - val_loss: 0.8085\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.80849, saving model to weights_best.h5\n",
      "Epoch 2/6\n",
      " - 5s - loss: 0.4999 - val_loss: 0.4184\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.80849 to 0.41843, saving model to weights_best.h5\n",
      "Epoch 3/6\n",
      " - 5s - loss: 0.3913 - val_loss: 0.3600\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.41843 to 0.35997, saving model to weights_best.h5\n",
      "Epoch 4/6\n",
      " - 6s - loss: 0.3294 - val_loss: 0.3546\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35997 to 0.35462, saving model to weights_best.h5\n",
      "Epoch 5/6\n",
      " - 5s - loss: 0.2990 - val_loss: 0.3234\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35462 to 0.32344, saving model to weights_best.h5\n",
      "Epoch 6/6\n",
      " - 5s - loss: 0.2605 - val_loss: 0.2946\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.32344 to 0.29456, saving model to weights_best.h5\n",
      "Optimal F1: 0.8577 at threshold: 0.4660\n",
      "Train on 4934 samples, validate on 1234 samples\n",
      "Epoch 1/6\n",
      " - 6s - loss: 0.7487 - val_loss: 0.7016\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70161, saving model to weights_best.h5\n",
      "Epoch 2/6\n",
      " - 5s - loss: 0.5127 - val_loss: 0.4291\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.70161 to 0.42912, saving model to weights_best.h5\n",
      "Epoch 3/6\n",
      " - 5s - loss: 0.3926 - val_loss: 0.3741\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.42912 to 0.37412, saving model to weights_best.h5\n",
      "Epoch 4/6\n",
      " - 5s - loss: 0.3367 - val_loss: 0.3174\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.37412 to 0.31743, saving model to weights_best.h5\n",
      "Epoch 5/6\n",
      " - 5s - loss: 0.2974 - val_loss: 0.3182\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.31743\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0006000000284984708.\n",
      "Epoch 6/6\n",
      " - 5s - loss: 0.2598 - val_loss: 0.2877\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.31743 to 0.28765, saving model to weights_best.h5\n",
      "Optimal F1: 0.8641 at threshold: 0.2502\n",
      "Train on 4935 samples, validate on 1233 samples\n",
      "Epoch 1/6\n",
      " - 6s - loss: 0.7511 - val_loss: 0.7006\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.70058, saving model to weights_best.h5\n",
      "Epoch 2/6\n",
      " - 5s - loss: 0.5339 - val_loss: 0.5356\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.70058 to 0.53558, saving model to weights_best.h5\n",
      "Epoch 3/6\n",
      " - 5s - loss: 0.4211 - val_loss: 0.4570\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.53558 to 0.45697, saving model to weights_best.h5\n",
      "Epoch 4/6\n",
      " - 5s - loss: 0.3666 - val_loss: 0.3505\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.45697 to 0.35048, saving model to weights_best.h5\n",
      "Epoch 5/6\n",
      " - 5s - loss: 0.3181 - val_loss: 0.2953\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35048 to 0.29535, saving model to weights_best.h5\n",
      "Epoch 6/6\n",
      " - 5s - loss: 0.2865 - val_loss: 0.2820\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.29535 to 0.28195, saving model to weights_best.h5\n",
      "Optimal F1: 0.8506 at threshold: 0.2785\n",
      "Train on 4935 samples, validate on 1233 samples\n",
      "Epoch 1/6\n",
      " - 6s - loss: 0.7159 - val_loss: 0.4915\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49152, saving model to weights_best.h5\n",
      "Epoch 2/6\n",
      " - 5s - loss: 0.4924 - val_loss: 0.3992\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49152 to 0.39925, saving model to weights_best.h5\n",
      "Epoch 3/6\n",
      " - 5s - loss: 0.3939 - val_loss: 0.3924\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.39925 to 0.39242, saving model to weights_best.h5\n",
      "Epoch 4/6\n",
      " - 5s - loss: 0.3384 - val_loss: 0.3842\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.39242 to 0.38419, saving model to weights_best.h5\n",
      "Epoch 5/6\n",
      " - 5s - loss: 0.2979 - val_loss: 0.3315\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.38419 to 0.33147, saving model to weights_best.h5\n",
      "Epoch 6/6\n",
      " - 5s - loss: 0.2647 - val_loss: 0.3075\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.33147 to 0.30752, saving model to weights_best.h5\n",
      "Optimal F1: 0.8508 at threshold: 0.1825\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(n_splits=5, random_state=10, shuffle=True)\n",
    "bestscore = []\n",
    "y_test = np.zeros((X_test.shape[0], ))\n",
    "for i, (train_index, valid_index) in enumerate(kfold.split(X, Y)):\n",
    "    X_train, X_val, Y_train, Y_val = X[train_index], X[valid_index], Y[train_index], Y[valid_index]\n",
    "    filepath=\"weights_best.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=1, min_lr=0.0001, verbose=2)\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose=2, mode='auto')\n",
    "    callbacks = [checkpoint, reduce_lr]\n",
    "    model = capsule()\n",
    "    if i == 0:print(model.summary()) \n",
    "    model.fit(X_train, Y_train, batch_size=512, epochs=6, validation_data=(X_val, Y_val), verbose=2, callbacks=callbacks, \n",
    "             )\n",
    "    model.load_weights(filepath)\n",
    "    y_pred = model.predict([X_val], batch_size=1024, verbose=2)\n",
    "    y_test += np.squeeze(model.predict([X_test], batch_size=1024, verbose=2))/5\n",
    "    f1, threshold = f1_smart(np.squeeze(Y_val), np.squeeze(y_pred))\n",
    "    print('Optimal F1: {:.4f} at threshold: {:.4f}'.format(f1, threshold))\n",
    "    bestscore.append(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reshape((-1, 1))\n",
    "pred_test_y = (y_test>np.mean(bestscore)).astype(int)\n",
    "predictions = [round(value[0]) for value in pred_test_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(659, 109, 50, 416)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(test[\"target\"], predictions).ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8711507293354943"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test[\"target\"], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"preds\"] = predictions\n",
    "test.to_csv(\"predictions-lstm.tsv\", sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sentence_similarity_3.6]",
   "language": "python",
   "name": "conda-env-sentence_similarity_3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
